import cv2
import numpy as np
from fer import FER
import pyaudio

# Initialize the FER detector
detector = FER()

# Use the corrected path to the Haar Cascade XML file
haar_cascade_path = r"C:\Users\Rakshitha\Downloads\haarcascade_frontalface_default.xml" 
face_cascade = cv2.CascadeClassifier(haar_cascade_path)

# Initialize audio capture
p = pyaudio.PyAudio()
stream = p.open(format=pyaudio.paInt16, channels=1, rate=44100, input=True, frames_per_buffer=1024)

def get_audio_level(data):
    """Calculate audio level from the raw audio data."""
    audio_data = np.frombuffer(data, dtype=np.int16)
    return np.abs(audio_data).mean()

# Start video capture from the webcam
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("Error: Could not open webcam.")
    exit()

while True:
    # Read a frame from the webcam
    ret, frame = cap.read()

    if not ret:
        print("Error: Could not read frame.")
        break

    # Convert the frame to grayscale (Haar Cascade works with grayscale images)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the frame
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # Draw rectangles around the detected faces and predict emotions
    for (x, y, w, h) in faces:
        # Extract the face ROI
        roi = frame[y:y + h, x:x + w]

        # Detect emotions in the face ROI
        emotions = detector.detect_emotions(roi)
        if emotions:
            # Get the top emotion
            emotion, score = detector.top_emotion(roi)

            if emotion and score is not None:
                # Draw the rectangle around the face
                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
                # Display the emotion label with smaller red text
                label = f"{emotion}: {score * 100:.2f}%"
                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)  # Smaller red text
            else:
                # Handle cases where emotion or score is None
                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
                cv2.putText(frame, "Emotion not detected", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)  # Smaller red text
        else:
            # Handle cases where no emotions are detected
            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
            cv2.putText(frame, "No emotion detected", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)  # Smaller red text

    # Capture audio data
    audio_data = stream.read(1024, exception_on_overflow=False)
    audio_level = get_audio_level(audio_data)

    # Display audio level with smaller red text
    audio_label = f"Audio Level: {audio_level:.2f}"
    cv2.putText(frame, audio_label, (10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)  # Smaller red text

    # Display the frame with the face detection, emotion labels, and audio level
    cv2.imshow('Face Detection', frame)

    # Break the loop if the user presses the 'q' key
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
stream.stop_stream()
stream.close()
p.terminate()
cv2.destroyAllWindows()
